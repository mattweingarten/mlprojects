{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2\n",
    "Current considerations:\n",
    "1. Current score ~0.773 HardBaseline : 0.772478169274\n",
    "- Experiment with different parametrizations and models (currently only one per subtask)\n",
    "- Experiment with preprocessing (maybe I'll try to use PCA and see if it can make a difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries - needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed libraries\n",
    "import pandas as pd #Pandas\n",
    "import numpy as np #Numpy\n",
    "import sklearn #Sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "#libraries needed for preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Libraries needed for imputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "#Libraries needed for models\n",
    "#subtasks 1 and 2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "#subtask3\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "#Libraries needed for plotters\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Libraries needed for scoring\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "## MATT Libraries\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import linear_model\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.set_printoptions(precision=5,suppress=True, linewidth=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation functions - needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "1. check_all_nan \n",
    "- zeroize\n",
    "- scale\n",
    "- get_next_val\n",
    "- patient_pca\n",
    "- get_patient_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_nan(vector):\n",
    "    checker = np.vectorize(np.isnan)\n",
    "    return np.all(checker(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroize(vector):\n",
    "    for i in range(vector.size):\n",
    "        vector[i] = 0\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data):\n",
    "    n,w = data.shape\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    scaler.fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_val(vector,index):\n",
    "    count = 0\n",
    "    for j in range(vector.size - index):\n",
    "        if(np.isnan(vector[index + j]) == False):\n",
    "            return (count,vector[index + j])\n",
    "        count += 1\n",
    "    return (count,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_pca(data,c):\n",
    "    pca = PCA(n_components=c).fit(data)\n",
    "    return np.reshape(pca.components_,(pca.components_.size,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_matrix(raw_data,i):\n",
    "    n,w = raw_data.shape\n",
    "    return raw_data[(12 * i): (12 *(i+1))][:,3:w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation functions\n",
    "1. interp\n",
    "- nan_imputer - imputes data using sklearn InterativeImputer function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp(vector):\n",
    "   if(check_all_nan(vector)):\n",
    "       return zeroize(vector)\n",
    "   prev_val = np.nan\n",
    "   for i in range(vector.size):\n",
    "        nans,next_val = get_next_val(vector, i)\n",
    "        if(np.isnan(vector[i])):\n",
    "            if(np.isnan(prev_val)):\n",
    "                vector[i] = next_val\n",
    "            elif(np.isnan(next_val)):\n",
    "                vector[i] = prev_val\n",
    "            else:\n",
    "                temp = prev_val +  (next_val - prev_val)/ (nans + 1)\n",
    "                vector[i] = temp\n",
    "                prev_val = temp\n",
    "        else:\n",
    "            prev_val = vector[i]\n",
    "   return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_imputer(nds,method):\n",
    "    \"\"\"\n",
    "    Given a dataset removes NaNs using\n",
    "    Parameters:\n",
    "    Input nds - Numpy array: dataset\n",
    "    Input method - method of imputation to use\n",
    "    Output nds_xnan - Numpy array: dataset without NaNs\n",
    "    \"\"\"\n",
    "    if method==1:#Sklearn: IterativeImputer, removes NaN considering other features\n",
    "        imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "        imp.fit(nds)\n",
    "        IterativeImputer(random_state=0)\n",
    "        nds_xnan = imp.transform(nds)\n",
    "    return nds_xnan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction functions\n",
    "1. time_reduction\n",
    "- flatten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_reduction(nds,labels, time, method):\n",
    "    \"\"\"\n",
    "    Given a dataset containing data on consecutive hours outputs a row extracting information time features\n",
    "    Parameters:\n",
    "    Input nds - numpy dataset\n",
    "    Input labels - list labels of dataset\n",
    "    Input time - time in hours to compress data \n",
    "    Input method - method of reduction to use\n",
    "    Output nds_reduced - dataset compressed \n",
    "    \"\"\"\n",
    "    nds = pd.DataFrame(nds,columns=labels)\n",
    "    datalen = len(nds)\n",
    "    numpat = int(datalen / time) #number of patients\n",
    "    \n",
    "    if method==1:#average of values per patient\n",
    "        #Reduce by taking mean of columns for each patient\n",
    "        nds_reduced = nds.groupby('pid',sort=False,as_index=False).mean()\n",
    "        \n",
    "    elif method==2:#scoring method based on evolution of patient during stay\n",
    "        dss = np.array_split(nds,numpat,axis=0) #dataset split for each patient  \n",
    "        nds_reduced = []\n",
    "        flagr=True\n",
    "        for k in range(numpat):#for each patient\n",
    "            patient = dss[k]#select patient\n",
    "            npat = patient.to_numpy()\n",
    "            r_pat = []\n",
    "            for i in range (np.size(npat,1)):#for each label\n",
    "                cur_col = npat[:,i]\n",
    "                temp=0\n",
    "                ev = 0\n",
    "                flagn = True\n",
    "                for j in range(np.size(cur_col)):#for each row\n",
    "                    this=cur_col[j]\n",
    "                    if ~(np.isnan(this)):\n",
    "                        ev = ev + ((this-temp)*(j+1)/10) #evolution increasing on time\n",
    "                        temp=this\n",
    "                        flagn= False\n",
    "                if flagn:#if row is all NaN\n",
    "                    r_pat = np.append(r_pat,np.NaN)#insert NaN\n",
    "                else:#else\n",
    "                    r_pat = np.append(r_pat,ev)#insert evolution\n",
    "            if flagr:#if reduced set is empty\n",
    "                nds_reduced = np.append(nds_reduced,r_pat)#insert patient\n",
    "                flagr=False\n",
    "            else:#if at least one patient has been added\n",
    "                nds_reduced = np.vstack((nds_reduced, r_pat))#insert patient as row\n",
    "        #Transform to pandas for compatibility\n",
    "        nds_reduced=pd.DataFrame(nds_reduced,columns=labels)\n",
    "    #Reduce considering patient evolution during stay \n",
    "    return nds_reduced.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_pca(raw_data,c):\n",
    "    n,w = raw_data.shape\n",
    "    res = np.zeros((n/12,c*(w-2)))\n",
    "    temp = np.zeros((12,w-3))\n",
    "    for i in range(n/12):\n",
    "        temp = get_patient_matrix(raw_data,i)\n",
    "        for j in range (w-3):\n",
    "            temp[:,j] = interp(temp[:,j])\n",
    "\n",
    "        res[i][0] = raw_data[i * 12][2] /100\n",
    "        # print(patient_pca(temp,c))\n",
    "        res[i][1:c*w-2] = patient_pca(temp,c)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining functions\n",
    "1. clean_set\n",
    "- flatten_min_max_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_set(nds,labels,imp_method,time_method,sequence):\n",
    "    \"\"\"\n",
    "    Given a dataset containing data on consecutive hours outputs a row extracting information time features\n",
    "    Parameters:\n",
    "    Input nds - numpy dataset\n",
    "    Input labels - list labels of dataset\n",
    "    Input imp_method - time in hours to compress data \n",
    "    Input time_method - method of reduction to use\n",
    "    Input sequence - method of reduction to use\n",
    "    Output  - dataset compressed \n",
    "    \"\"\"\n",
    "    ds_clean = nds\n",
    "    if sequence:\n",
    "        ds_clean = time_reduction(ds_clean, labels, 12,time_method)\n",
    "        ds_clean = nan_imputer(ds_clean,imp_method)\n",
    "        ds_clean = pd.DataFrame(ds_clean, columns=labels)\n",
    "    else:\n",
    "        ds_clean = nan_imputer(ds_clean,imp_method)    \n",
    "        ds_clean = time_reduction(ds_clean, labels, 12,time_method)\n",
    "        ds_clean = pd.DataFrame(ds_clean, columns=labels)\n",
    "    \n",
    "        \n",
    "    return ds_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_min_max_slope(raw_data):\n",
    "    n,w =  raw_data.shape\n",
    "    c = w - 3\n",
    "    means = np.nanmean(raw_data,axis=0)\n",
    "    ndiv = int(n/12)\n",
    "    res = np.zeros((ndiv,1 + c * 3))\n",
    "    temp = np.zeros((12,c))\n",
    "    for i in range(ndiv):\n",
    "        temp = get_patient_matrix(raw_data,i)\n",
    "        for j in range (c):\n",
    "            temp[:,j] = interp(temp[:,j])\n",
    "        res[i][0] = raw_data[i * 12][2]\n",
    "        for j in range (c):\n",
    "            min = np.min(temp[:,j])\n",
    "            max = np.max(temp[:,j])\n",
    "            # if(min == 0):\n",
    "            #     min = means[j + 3]\n",
    "            # if(max == 0):\n",
    "            #     max = means[j + 3]\n",
    "            res[i][j*3+1] = min\n",
    "            res[i][j*3+2] = max\n",
    "            # res[i][j*3 + 2] = 0\n",
    "            res[i][j*3+3] = (max-min)/12\n",
    "    # print(res[0:5])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model choosing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fold_10(nds, f_nr, task):\n",
    "    \"\"\"\n",
    "    Given a dataset, outputs two subsets: training set and test set. Test sets is given by the f_nr-th partition of the dataset,\n",
    "    meanwhile the training set is the remaining of the dataset\n",
    "    Parameters:\n",
    "    Input ds - numpy dataset to partition\n",
    "    Input f_nr - number of the fold that will be the test set 1-10\n",
    "    Output (testset, trainingset) - tuple containing the test set and training set\n",
    "    \"\"\"\n",
    "    dss = np.array_split(nds,10,axis=0) #dataset split\n",
    "    testset = dss[f_nr] #test set\n",
    "    if task==2:\n",
    "        trainingset = np.hstack(np.delete(dss, f_nr, 0)) #training set \n",
    "    else:\n",
    "        trainingset = np.vstack(np.delete(dss, f_nr, 0)) #training set\n",
    "    return (testset, trainingset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold10_predict(models, ndsx, ndsy, ndsy_L, task):\n",
    "    \"\"\"\n",
    "    Do 10 fold cross validation on training set for one of the three subtasks\n",
    "    Parameters:\n",
    "    Input models - list of models \n",
    "    Input ndsx - training features\n",
    "    Input ndsy - training labels\n",
    "    Input ndsy_L - training labels headers\n",
    "    Input task - subtask to fold\n",
    "    Output nscores - list containing the scores of each fold\n",
    "    \"\"\"\n",
    "    ###Performing 10-fold Cross Validation for each Model\n",
    "    mlen = len(models) #Number of models\n",
    "    nscores = np.zeros((mlen,10)) #score of each fold\n",
    "    for j in range(10):\n",
    "        #Creating test set and training set from data set \n",
    "        if task==2:\n",
    "            (tes_x, trs_x) = data_fold_10(ndsx, j, task-1)\n",
    "            (tes_y, trs_y) = data_fold_10(ndsy, j, task)\n",
    "        else:\n",
    "            (tes_x, trs_x) = data_fold_10(ndsx, j, task)\n",
    "            (tes_y, trs_y) = data_fold_10(ndsy, j, task)\n",
    "\n",
    "        #Perform fitting and predicting for each model\n",
    "        for i in range(mlen):\n",
    "            models[i].fit(trs_x,trs_y)\n",
    "            if task==3:#if task is third we use predict\n",
    "                tes_yp = models[i].predict(tes_x)\n",
    "\n",
    "                #Transform into DataFrame for scoring\n",
    "                df_y = pd.DataFrame(tes_y, columns=ndsy_L)\n",
    "                df_yp = pd.DataFrame(tes_yp, columns=ndsy_L)\n",
    "                nscores[i,j] = scores(df_y,df_yp, task)\n",
    "            else:\n",
    "                tes_yp = predict_sigmoid(models[i],tes_x)\n",
    "                #Transform into DataFrame for scoring\n",
    "                df_y = pd.DataFrame(tes_y, columns=ndsy_L)\n",
    "                if task==2:\n",
    "                    df_yp = pd.DataFrame(tes_yp, columns=ndsy_L)\n",
    "                else:\n",
    "                    df_yp = pd.DataFrame(tes_yp, columns=ndsy_L)\n",
    "                nscores[i,j] = scores(df_y,df_yp, task)\n",
    "    return nscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(tes_y, tes_yp, task):\n",
    "    \"\"\"\n",
    "    Give score for one of the subtask\n",
    "    Parameters:\n",
    "    Input tes_y - training labels\n",
    "    Input tes_yp - predicted labels\n",
    "    Input task - subtask\n",
    "    Output score - score of the subtask\n",
    "    \"\"\"\n",
    "    if task==3:\n",
    "        VITALS = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "        score = np.mean([0.5 + 0.5 * np.maximum(0, metrics.r2_score(tes_y[entry], tes_yp[entry])) for entry in VITALS])\n",
    "    elif task==2:\n",
    "        score = metrics.roc_auc_score(tes_y['LABEL_Sepsis'], tes_yp['LABEL_Sepsis'])\n",
    "    else:\n",
    "        TESTS = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "        score = np.mean([metrics.roc_auc_score(tes_y[entry], tes_yp[entry]) for entry in TESTS])\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sigmoid(X):\n",
    "    xshape = X.shape\n",
    "    if len(xshape)==1:\n",
    "        n = xshape[0]\n",
    "        for i in range(n):\n",
    "            X[i] = sigmoid(X[i])\n",
    "    else:\n",
    "        n,m = xshape\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                X[i][j] = sigmoid(X[i][j])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sigmoid(clf,data):\n",
    "    return map_sigmoid(clf.decision_function(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtasks - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup for all subtasks\n",
    "\n",
    "#Extracting training labels and features\n",
    "dataset_y = pd.read_csv(\"train_labels.csv\")\n",
    "dataset_x = pd.read_csv(\"train_features.csv\")\n",
    "raw_data = np.genfromtxt(\"./train_features.csv\",delimiter=\",\",skip_header=1)\n",
    "\n",
    "\n",
    "#lists that contain header of dataset\n",
    "dataset_x_L = list(dataset_x) \n",
    "\n",
    "#Standard Scaler of dataset \n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "#scaler = RobustScaler()\n",
    "scaler.fit(dataset_x)\n",
    "scaled_data = scaler.transform(dataset_x)\n",
    "scaled_data = pd.DataFrame(scaled_data,columns=dataset_x_L)\n",
    "\n",
    "sfmms_data = scale(flatten_min_max_slope(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtask 1\n",
    "#Training labels\n",
    "dataset_y1 = dataset_y.loc[:,\"LABEL_BaseExcess\":\"LABEL_EtCO2\"] #Labels to be predicted in [0,1] range\n",
    "ds_y1_L = list(dataset_y1) #headers of labels\n",
    "ndsy1 = dataset_y1.to_numpy() #to numpy\n",
    "\n",
    "#Training Features\n",
    "#Cleaning dataset in order to remove NaNs and reduce dimensionality\n",
    "cs1 = clean_set(scaled_data,dataset_x_L,1,1,True)\n",
    "##Division for the prediction, probabilities divided from the real values\n",
    "ds_p1 = cs1.loc[:,\"Time\":\"pH\"] #reduced dataset for prediction, without pid\n",
    "\n",
    "#labels of datasets\n",
    "ds_p1_L = list(ds_p1)\n",
    "\n",
    "#transform into numpy\n",
    "nds_p1 = ds_p1.to_numpy()\n",
    "mnds_p1 = sfmms_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtask 2\n",
    "#Training labels\n",
    "dataset_y2 = dataset_y.loc[:,\"LABEL_Sepsis\"] #Labels to be predicted in [0,1] range\n",
    "ds_y2_L = [\"LABEL_Sepsis\"] #headers of labels\n",
    "ndsy2 = dataset_y2.to_numpy() #to numpy\n",
    "\n",
    "#Training Features\n",
    "cs2 = clean_set(scaled_data,dataset_x_L,1,1,True)\n",
    "ds_p2 = cs2.loc[:,\"Time\":\"pH\"] #reduced dataset for prediction, without pid\n",
    "ds_p2_L = list(ds_p2)\n",
    "nds_p2 = ds_p2.to_numpy()\n",
    "mnds_p2 = sfmms_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtask 3\n",
    "#Training labels\n",
    "dataset_y3 = dataset_y.loc[:,\"LABEL_RRate\":\"LABEL_Heartrate\"] #Labels to be predicted in [0,1] range\n",
    "ds_y3_L = list(dataset_y3) #headers of labels\n",
    "ndsy3 = dataset_y3.to_numpy() #to numpy\n",
    "\n",
    "#Training Features\n",
    "cs3 = clean_set(scaled_data,dataset_x_L,1,1,True)\n",
    "ds_p3 = cs3.loc[:,\"Time\":\"pH\"] #reduced dataset for prediction, without pid\n",
    "ds_p3_L = list(ds_p3)\n",
    "nds_p3 = ds_p3.to_numpy()\n",
    "mnds_p3 = sfmms_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask1 training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model set used for training\n",
    "svcms = SVC(kernel='linear', \n",
    "            decision_function_shape='ovr', \n",
    "            gamma='auto', \n",
    "            probability=True, \n",
    "            max_iter=1000)\n",
    "matt_svcms = svm.SVC(kernel='linear',\n",
    "                     decision_function_shape='ovr', \n",
    "                     C=0.01,\n",
    "                     max_iter=10000)\n",
    "models1 = []\n",
    "#1 - Sklearn: OnveVsRestClassifier using svcms\n",
    "#models1 = np.append(models1,OneVsRestClassifier(svcms))\n",
    "#2 - Sklearn: OneVsOneClassifier using svcms\n",
    "#models1 = np.append(models1,OneVsOneClassifier(svcms))\n",
    "#3 - Sklearn: OnveVsRestClassifier using svcms\n",
    "models1 = np.append(models1,OneVsRestClassifier(matt_svcms))\n",
    "#4 - Sklearn: OneVsOneClassifier using svcms\n",
    "#models1 = np.append(models1,OneVsOneClassifier(matt_svcms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.75848</td>\n",
       "      <td>0.751267</td>\n",
       "      <td>0.75307</td>\n",
       "      <td>0.744522</td>\n",
       "      <td>0.736351</td>\n",
       "      <td>0.747362</td>\n",
       "      <td>0.745913</td>\n",
       "      <td>0.741246</td>\n",
       "      <td>0.727256</td>\n",
       "      <td>0.755588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1        2         3         4         5         6  \\\n",
       "0  0.75848  0.751267  0.75307  0.744522  0.736351  0.747362  0.745913   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.741246  0.727256  0.755588  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Predictions\n",
    "s1 = fold10_predict(models1,mnds_p1,ndsy1,ds_y1_L,1)\n",
    "pd.DataFrame(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.746106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.746106"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_s1 = np.average(s1,axis=1)\n",
    "pd.DataFrame(avg_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask2 - training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model list used for predictions\n",
    "models2 = []\n",
    "matt_svcms = svm.SVC(kernel='sigmoid',\n",
    "                     decision_function_shape='ovr', \n",
    "                     C=0.01,\n",
    "                     max_iter=10000)\n",
    "#0 - Sklearn: SVC \n",
    "#models2 = np.append(models2,SVC(kernel='sigmoid',\n",
    "#                                gamma='auto',\n",
    "#                                probability=True,\n",
    "#                                max_iter=1000))\n",
    "#1 - Sklearn: SVR\n",
    "#models2 = np.append(models2,matt_svcms)\n",
    "#from sklearn.svm import LinearSVC\n",
    "models2 = np.append(models2,sklearn.svm.LinearSVC(random_state=0,dual=False, tol=1e-10,max_iter=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predictions\n",
    "s2 = fold10_predict(models2,mnds_p2,ndsy2,ds_y2_L,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.658932</td>\n",
       "      <td>0.713951</td>\n",
       "      <td>0.724796</td>\n",
       "      <td>0.688253</td>\n",
       "      <td>0.735791</td>\n",
       "      <td>0.711369</td>\n",
       "      <td>0.730331</td>\n",
       "      <td>0.742531</td>\n",
       "      <td>0.691106</td>\n",
       "      <td>0.683634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.658932  0.713951  0.724796  0.688253  0.735791  0.711369  0.730331   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.742531  0.691106  0.683634  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.708069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.708069"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_s2 = np.average(s2,axis=1)\n",
    "pd.DataFrame(avg_s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask3 - training models\n",
    "I included only ridge as it is relatively fast and produces ~0.745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model set used for training\n",
    "models3 = []\n",
    "#0 - Sklearn: Ridge regression function with alpha 1\n",
    "models3 = np.append(models3, linear_model.RidgeCV(alphas=[10*a for a in range(1,10)],\n",
    "                                                 fit_intercept=True,\n",
    "                                                 gcv_mode='svd'\n",
    "                                                 )) \n",
    "#1 - Sklearn: Multi Task Lasso Cross Validation on alphas\n",
    "#models3 = np.append(models3,linear_model.MultiTaskLassoCV(cv=10,\n",
    "#                                                          alphas=[10**a for a in range(-1,10)],\n",
    "#                                                          fit_intercept=True,\n",
    "#                                                          max_iter=1000))\n",
    "#2 Sklearn: Multitask Elastic Net with Cross Validation\n",
    "#models3 = np.append(models3,linear_model.MultiTaskElasticNetCV(cv=10,random_state=0,max_iter=10000))\n",
    "#models3 = np.append(models3,OneVsRest(linear_model.SGDRegressor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predictions\n",
    "s3 = fold10_predict(models3, nds_p3, ndsy3, ds_y3_L, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.748734</td>\n",
       "      <td>0.746099</td>\n",
       "      <td>0.74074</td>\n",
       "      <td>0.74454</td>\n",
       "      <td>0.732383</td>\n",
       "      <td>0.767828</td>\n",
       "      <td>0.743956</td>\n",
       "      <td>0.732851</td>\n",
       "      <td>0.749092</td>\n",
       "      <td>0.755045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1        2        3         4         5         6  \\\n",
       "0  0.748734  0.746099  0.74074  0.74454  0.732383  0.767828  0.743956   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.732851  0.749092  0.755045  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.746127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.746127"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_s3 = np.average(s3,axis=1)\n",
    "pd.DataFrame(avg_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7334338851901423"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([avg_s1, avg_s2, avg_s3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testset prediction - in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Dummy code to combine three results\n",
    "best_1 = models1[0]#Dummy\n",
    "best_2 = models2[0]#Dummy\n",
    "best_3 = models3[0]#Dummy\n",
    "\n",
    "#training best model on entire dataset\n",
    "best_1.fit(mnds_p1,ndsy1)\n",
    "best_2.fit(mnds_p2,ndsy2)\n",
    "best_3.fit(nds_p3,ndsy3)\n",
    "\n",
    "#extract dataset to predict\n",
    "testset_x = pd.read_csv(\"test_features.csv\")\n",
    "testset_x_L = list(testset_x)\n",
    "test_x = testset_x.to_numpy()\n",
    "\n",
    "#Standard Scaler of dataset \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(testset_x)\n",
    "scaled_test = scaler.transform(testset_x)\n",
    "scaled_test = pd.DataFrame(scaled_test,columns=dataset_x_L)\n",
    "\n",
    "test_raw = np.genfromtxt(\"./test_features.csv\",delimiter=\",\",skip_header=1)\n",
    "\n",
    "#cleaning data\n",
    "test12_x = scale(flatten_min_max_slope(test_raw))\n",
    "#test2_x = clean_set(test_x,testset_x_L,1,1,True)\n",
    "test3_x = clean_set(scaled_test,testset_x_L,1,1,True)\n",
    "\n",
    "#reduced dataset for prediction, without pid\n",
    "#ctes1 = test1_x.loc[:,\"Time\":\"pH\"] \n",
    "#ctes2 = test2_x.loc[:,\"Time\":\"pH\"]\n",
    "ctes3 = test3_x.loc[:,\"Time\":\"pH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction using best models for each subtask\n",
    "pred1 = predict_sigmoid(best_1,test12_x)\n",
    "pred2 = predict_sigmoid(best_2,test12_x)\n",
    "pred3 = best_3.predict(ctes3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion to df\n",
    "pred1 = pd.DataFrame(pred1,columns=ds_y1_L)\n",
    "pred2 = pd.DataFrame(pred2,columns=ds_y2_L)\n",
    "pred3 = pd.DataFrame(pred3,columns=ds_y3_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "      <th>LABEL_Sepsis</th>\n",
       "      <th>LABEL_RRate</th>\n",
       "      <th>LABEL_ABPm</th>\n",
       "      <th>LABEL_SpO2</th>\n",
       "      <th>LABEL_Heartrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.884105</td>\n",
       "      <td>0.268751</td>\n",
       "      <td>0.999823</td>\n",
       "      <td>0.999745</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.275473</td>\n",
       "      <td>0.264228</td>\n",
       "      <td>0.208356</td>\n",
       "      <td>0.266602</td>\n",
       "      <td>0.250971</td>\n",
       "      <td>0.336695</td>\n",
       "      <td>14.090522</td>\n",
       "      <td>82.107640</td>\n",
       "      <td>98.610515</td>\n",
       "      <td>83.488713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.285673</td>\n",
       "      <td>0.268605</td>\n",
       "      <td>0.271105</td>\n",
       "      <td>0.268496</td>\n",
       "      <td>0.273318</td>\n",
       "      <td>0.275753</td>\n",
       "      <td>0.268795</td>\n",
       "      <td>0.269316</td>\n",
       "      <td>0.269582</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.283785</td>\n",
       "      <td>18.245793</td>\n",
       "      <td>88.779223</td>\n",
       "      <td>94.906971</td>\n",
       "      <td>102.966801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10003</td>\n",
       "      <td>0.286959</td>\n",
       "      <td>0.268527</td>\n",
       "      <td>0.267113</td>\n",
       "      <td>0.270399</td>\n",
       "      <td>0.267692</td>\n",
       "      <td>0.264925</td>\n",
       "      <td>0.268274</td>\n",
       "      <td>0.476533</td>\n",
       "      <td>0.267850</td>\n",
       "      <td>0.250788</td>\n",
       "      <td>0.272699</td>\n",
       "      <td>18.694166</td>\n",
       "      <td>83.906469</td>\n",
       "      <td>97.814410</td>\n",
       "      <td>91.725402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10004</td>\n",
       "      <td>0.138299</td>\n",
       "      <td>0.268554</td>\n",
       "      <td>0.261414</td>\n",
       "      <td>0.259220</td>\n",
       "      <td>0.257889</td>\n",
       "      <td>0.269630</td>\n",
       "      <td>0.268992</td>\n",
       "      <td>0.244974</td>\n",
       "      <td>0.268761</td>\n",
       "      <td>0.259458</td>\n",
       "      <td>0.278446</td>\n",
       "      <td>16.554891</td>\n",
       "      <td>72.649804</td>\n",
       "      <td>95.798655</td>\n",
       "      <td>88.254371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10005</td>\n",
       "      <td>0.210558</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>0.265864</td>\n",
       "      <td>0.264186</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.268360</td>\n",
       "      <td>0.268489</td>\n",
       "      <td>0.261849</td>\n",
       "      <td>0.268877</td>\n",
       "      <td>0.227244</td>\n",
       "      <td>0.285449</td>\n",
       "      <td>19.317833</td>\n",
       "      <td>75.179280</td>\n",
       "      <td>95.986274</td>\n",
       "      <td>61.304061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12659</td>\n",
       "      <td>9989</td>\n",
       "      <td>0.452198</td>\n",
       "      <td>0.268557</td>\n",
       "      <td>0.270673</td>\n",
       "      <td>0.264436</td>\n",
       "      <td>0.266971</td>\n",
       "      <td>0.281353</td>\n",
       "      <td>0.267546</td>\n",
       "      <td>0.269880</td>\n",
       "      <td>0.268961</td>\n",
       "      <td>0.230196</td>\n",
       "      <td>0.329972</td>\n",
       "      <td>19.685199</td>\n",
       "      <td>80.626953</td>\n",
       "      <td>95.701925</td>\n",
       "      <td>103.366045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12660</td>\n",
       "      <td>9991</td>\n",
       "      <td>0.532191</td>\n",
       "      <td>0.268451</td>\n",
       "      <td>0.266830</td>\n",
       "      <td>0.259680</td>\n",
       "      <td>0.266712</td>\n",
       "      <td>0.261873</td>\n",
       "      <td>0.265663</td>\n",
       "      <td>0.105029</td>\n",
       "      <td>0.268277</td>\n",
       "      <td>0.238321</td>\n",
       "      <td>0.316115</td>\n",
       "      <td>18.281019</td>\n",
       "      <td>93.856425</td>\n",
       "      <td>98.708335</td>\n",
       "      <td>75.464507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12661</td>\n",
       "      <td>9992</td>\n",
       "      <td>0.541797</td>\n",
       "      <td>0.268484</td>\n",
       "      <td>0.267771</td>\n",
       "      <td>0.265269</td>\n",
       "      <td>0.262578</td>\n",
       "      <td>0.243247</td>\n",
       "      <td>0.267215</td>\n",
       "      <td>0.417678</td>\n",
       "      <td>0.266818</td>\n",
       "      <td>0.239790</td>\n",
       "      <td>0.282550</td>\n",
       "      <td>18.663326</td>\n",
       "      <td>70.735754</td>\n",
       "      <td>97.412140</td>\n",
       "      <td>83.687539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12662</td>\n",
       "      <td>9994</td>\n",
       "      <td>0.994535</td>\n",
       "      <td>0.268481</td>\n",
       "      <td>0.739315</td>\n",
       "      <td>0.684714</td>\n",
       "      <td>0.720919</td>\n",
       "      <td>0.990964</td>\n",
       "      <td>0.263836</td>\n",
       "      <td>0.946843</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>0.221694</td>\n",
       "      <td>0.424816</td>\n",
       "      <td>16.031283</td>\n",
       "      <td>85.844212</td>\n",
       "      <td>98.422374</td>\n",
       "      <td>97.363627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12663</td>\n",
       "      <td>9997</td>\n",
       "      <td>0.824494</td>\n",
       "      <td>0.268512</td>\n",
       "      <td>0.269395</td>\n",
       "      <td>0.265435</td>\n",
       "      <td>0.266184</td>\n",
       "      <td>0.279968</td>\n",
       "      <td>0.268229</td>\n",
       "      <td>0.255257</td>\n",
       "      <td>0.268917</td>\n",
       "      <td>0.228913</td>\n",
       "      <td>0.307917</td>\n",
       "      <td>18.251504</td>\n",
       "      <td>75.808021</td>\n",
       "      <td>98.187022</td>\n",
       "      <td>86.737911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
       "0          0          0.884105          0.268751   0.999823   \n",
       "1      10001          0.285673          0.268605   0.271105   \n",
       "2      10003          0.286959          0.268527   0.267113   \n",
       "3      10004          0.138299          0.268554   0.261414   \n",
       "4      10005          0.210558          0.268519   0.265864   \n",
       "...      ...               ...               ...        ...   \n",
       "12659   9989          0.452198          0.268557   0.270673   \n",
       "12660   9991          0.532191          0.268451   0.266830   \n",
       "12661   9992          0.541797          0.268484   0.267771   \n",
       "12662   9994          0.994535          0.268481   0.739315   \n",
       "12663   9997          0.824494          0.268512   0.269395   \n",
       "\n",
       "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
       "0                0.999745               0.999934       0.275473   \n",
       "1                0.268496               0.273318       0.275753   \n",
       "2                0.270399               0.267692       0.264925   \n",
       "3                0.259220               0.257889       0.269630   \n",
       "4                0.264186               0.266400       0.268360   \n",
       "...                   ...                    ...            ...   \n",
       "12659            0.264436               0.266971       0.281353   \n",
       "12660            0.259680               0.266712       0.261873   \n",
       "12661            0.265269               0.262578       0.243247   \n",
       "12662            0.684714               0.720919       0.990964   \n",
       "12663            0.265435               0.266184       0.279968   \n",
       "\n",
       "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
       "0             0.264228    0.208356                0.266602     0.250971   \n",
       "1             0.268795    0.269316                0.269582     0.253968   \n",
       "2             0.268274    0.476533                0.267850     0.250788   \n",
       "3             0.268992    0.244974                0.268761     0.259458   \n",
       "4             0.268489    0.261849                0.268877     0.227244   \n",
       "...                ...         ...                     ...          ...   \n",
       "12659         0.267546    0.269880                0.268961     0.230196   \n",
       "12660         0.265663    0.105029                0.268277     0.238321   \n",
       "12661         0.267215    0.417678                0.266818     0.239790   \n",
       "12662         0.263836    0.946843                0.266389     0.221694   \n",
       "12663         0.268229    0.255257                0.268917     0.228913   \n",
       "\n",
       "       LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate  \n",
       "0          0.336695    14.090522   82.107640   98.610515        83.488713  \n",
       "1          0.283785    18.245793   88.779223   94.906971       102.966801  \n",
       "2          0.272699    18.694166   83.906469   97.814410        91.725402  \n",
       "3          0.278446    16.554891   72.649804   95.798655        88.254371  \n",
       "4          0.285449    19.317833   75.179280   95.986274        61.304061  \n",
       "...             ...          ...         ...         ...              ...  \n",
       "12659      0.329972    19.685199   80.626953   95.701925       103.366045  \n",
       "12660      0.316115    18.281019   93.856425   98.708335        75.464507  \n",
       "12661      0.282550    18.663326   70.735754   97.412140        83.687539  \n",
       "12662      0.424816    16.031283   85.844212   98.422374        97.363627  \n",
       "12663      0.307917    18.251504   75.808021   98.187022        86.737911  \n",
       "\n",
       "[12664 rows x 16 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding the pids and assemble final prediction\n",
    "pids = time_reduction(testset_x,list(testset_x),12,1)\n",
    "pd.DataFrame(pids)\n",
    "pids = pd.DataFrame(pids[:,0],columns=['pid'])\n",
    "pred = pd.concat([pids.astype(int),pred1,pred2, pred3], axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine predictions into one dataframe\n",
    "#output, 3 digit floats\n",
    "pred.to_csv('prediction.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check format\n",
    "df_submission = pd.read_csv('prediction.zip')\n",
    "df_sample = pd.read_csv('sample.zip')\n",
    "df_submission.shape == df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not used functions, I might apply them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(raw):\n",
    "    n,m = raw.shape\n",
    "    means = np.nanmean(raw,axis=0)\n",
    "    stds = np.nanstd(raw,axis=0)\n",
    "    for j in range(m):\n",
    "        for i in range(n):\n",
    "            if (math.isnan(raw[i][j])):\n",
    "                # for mean:\n",
    "                raw[i][j] = means[j]\n",
    "                #for zero\n",
    "                # raw[i][j] = 0\n",
    "        scaler = StandardScaler(copy=False)\n",
    "        scaler.fit(raw)\n",
    "        raw = scaler.transform(raw)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(raw_data):\n",
    "    n,w = raw_data.shape\n",
    "    res = np.zeros((n/12, 1 + 12 * (w - 2)))\n",
    "    tw = w-2\n",
    "    for i in range(res.shape[0]):\n",
    "        res[i][0] = raw_data[i * 12][2]\n",
    "        temp = np.zeros((12 * tw))\n",
    "        for j in range(12):\n",
    "            res[i][1 + j * tw] = raw_data[i*12 + j][1]\n",
    "            res[i][(1 + j * tw + 1) :  (1 + (j+1) * tw)  ]  = raw_data[i*12+j][3:w]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "        raw_data = np.genfromtxt(\"./train_features.csv\",delimiter=\",\",skip_header=1)\n",
    "        # raw_data = missing_values(raw_data)\n",
    "        # samples = flatten_pca(raw_data,1)\n",
    "        samples = scale(flatten_min_max_slope(raw_data))\n",
    "        raw_labels = np.genfromtxt(\"./train_labels.csv\",delimiter=\",\",skip_header=1)\n",
    "        labels = raw_labels[:,1:12]\n",
    "        return samples,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    return OneVsRestClassifier(svm.SVC(\n",
    "                                kernel='linear',\n",
    "                                decision_function_shape='ovr',\n",
    "                                C=0.01,\n",
    "                                max_iter=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_csv(clf):\n",
    "    raw_data = np.genfromtxt(\"./train_features.csv\",delimiter=\",\",skip_header=1)\n",
    "    raw_data = missing_values(raw_data)\n",
    "    data = flatten(raw_data)\n",
    "    return predict(data,clf)\n",
    "    return restructure_predict(clf.predict_proba(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc(samples,labels):\n",
    "    return cross_val_score(model(),samples,labels, scoring='roc_auc',cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    set_up_start = time.time()\n",
    "    samples,labels = setup()\n",
    "    n,m = samples.shape\n",
    "    print(\"--------------------------------\\ndone setup\")\n",
    "    train_start = time.time()\n",
    "    setup_duration = train_start - set_up_start\n",
    "    print (\"Preprocessing takes: %.1f seconds\\n--------------------------------\"  % setup_duration)\n",
    "    roc = get_roc(samples,labels)\n",
    "    print(roc)\n",
    "    print(\"--------------------------------\\ndone training\")\n",
    "    scoring_start = time.time()\n",
    "    train_duration = scoring_start - train_start\n",
    "    print(\"Training takes: %.1f seconds\\n--------------------------------\"  % train_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
